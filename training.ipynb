{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3907428b-5edb-472b-a29b-f7facb915cfb",
      "metadata": {
        "id": "3907428b-5edb-472b-a29b-f7facb915cfb"
      },
      "source": [
        "# Climate Hack.AI 2022\n",
        "\n",
        "Thank you for participating in Climate Hack.AI 2022!\n",
        "\n",
        "Your hard work in helping to advance the state of the art in HRV satellite imagery nowcasting could be incredibly impactful in the broader fight against climate change. Within the UK electricty grid alone, your model could contribute to a reduction in carbon emissions of up to 100 kilotonnes a year.\n",
        "\n",
        "Happy model training!\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you begin, please ensure that you have the following packages installed.\n",
        "\n",
        "* `xarray` for representing n-dimensional labelled arrays\n",
        "* `zarr` for loading the Zarr dataset\n",
        "* `gcsfs` for loading from Google Cloud Storage:  used implicitly by xarray)\n",
        "* `dask` used under the hood by xarray to enable parallel processing of datasets too large to fit into RAM\n",
        "* `torch` (PyTorch) for training a model\n",
        "* `pytorch_msssim` to use a multi-scale structural similarity index measure-based loss function\n",
        "* `matplotlib` for plotting\n",
        "\n",
        "If you want to install them using `pip`, you can run the following command (replacing `python` with `python3` as appropriate):\n",
        "\n",
        "```bash\n",
        "$ python -m pip install numpy matplotlib zarr xarray ipykernel gcsfs fsspec dask cartopy\n",
        "```\n",
        "\n",
        "You will then need to install the appropriate PyTorch distribution for your system, the instructions for which may be found [here](https://pytorch.org/get-started/locally/).\n",
        "\n",
        "Otherwise, if you are familar with `conda`, you may want to run the following commands to set up a `conda` environment for Climate Hack.AI:\n",
        "\n",
        "```bash\n",
        "$ conda create --name climatehack python=3.9 numpy matplotlib zarr xarray ipykernel gcsfs fsspec dask cartopy\n",
        "$ conda activate climatehack\n",
        "$ conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "$ conda install -c pvlib pvlib\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642c976a",
      "metadata": {},
      "source": [
        "## Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccebc6d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import xarray as xr\n",
        "from numpy import float32\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9015733",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import ClimateHackDataset\n",
        "from loss import MS_SSIMLoss\n",
        "# from submission.basemodel import Model\n",
        "# import torch.nn as nn\n",
        "from submission.seq2seqConvLSTM import Model\n",
        "# from submission.convLSTM import ConvLSTMCell\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5bf592d",
      "metadata": {},
      "source": [
        "## Loading the dataset\n",
        "\n",
        "The 108GB HRV (\"high resolution visible\") satellite imagery dataset may be loaded directly from Google Cloud Public Datasets using `xarray`, `zarr`, `gcsfs` and `dask`. Alternatively a local copy can be downloaded and loaded instead for better performance. The pixel values represent calibrated top-of-atmosphere bidirectional irradiance values mapped to the range `[0, 1023]`. Read more about the data on the [Google Cloud Platform](https://console.cloud.google.com/marketplace/product/bigquery-public-data/eumetsat-seviri-rss-hrv-uk).\n",
        "\n",
        "### Loading the dataset from the cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830e5ace-8a6c-402d-95e8-0ea1d68222b2",
      "metadata": {
        "id": "830e5ace-8a6c-402d-95e8-0ea1d68222b2"
      },
      "outputs": [],
      "source": [
        "SATELLITE_ZARR_PATH = \"gs://public-datasets-eumetsat-solar-forecasting/satellite/EUMETSAT/SEVIRI_RSS/v3/eumetsat_seviri_hrv_uk.zarr\"\n",
        "\n",
        "dataset = xr.open_dataset(\n",
        "    SATELLITE_ZARR_PATH, \n",
        "    engine=\"zarr\",\n",
        "    chunks=\"auto\",  # Load the data as a Dask array\n",
        ")\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a56fa83",
      "metadata": {},
      "source": [
        "### Loading a local copy of the dataset\n",
        "\n",
        "Loading data from the cloud in this way can often be slow, so it is likely that you will want to download a local copy of the 104GiB dataset, at least in part.\n",
        "\n",
        "To do this, first install `gsutil` by following [these instructions](https://cloud.google.com/storage/docs/gsutil_install).\n",
        "\n",
        "Then, run the following command to start downloading the dataset into your current working directory:\n",
        "\n",
        "```bash\n",
        "$ gsutil -m cp -rJn gs://public-datasets-eumetsat-solar-forecasting/satellite/EUMETSAT/SEVIRI_RSS/v3/eumetsat_seviri_hrv_uk.zarr .\n",
        "```\n",
        "\n",
        "The `-n` no-clobber flag will skip previously downloaded files if the command is run multiple times, so you can download as much of the dataset as you want in stages.\n",
        "\n",
        "Then, change the `SATELLITE_ZARR_PATH` to point to the downloaded data on your computer.\n",
        "\n",
        "```python\n",
        "SATELLITE_ZARR_PATH = \"eumetsat_seviri_hrv_uk.zarr\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66f359e",
      "metadata": {},
      "source": [
        "## Visualising the data\n",
        "\n",
        "You may be curious what these 1841x891 \"high resolution visible\" satellite images actually look like! \n",
        "\n",
        "Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793060be",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[\"data\"].sel(time=\"2020-06-01 12:00\").plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff241d5",
      "metadata": {},
      "source": [
        "As part of your challenge, you are given twelve 128x128 images taken five minutes apart corresponding to some region inside the 1841x891 image above, e.g."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2fcaf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 12, figsize=(15,3))\n",
        "\n",
        "for i, d in enumerate([\"2020-07-04 12:00\", \"2020-07-04 12:05\", \"2020-07-04 12:10\", \"2020-07-04 12:15\", \"2020-07-04 12:20\", \"2020-07-04 12:25\", \"2020-07-04 12:30\", \"2020-07-04 12:35\", \"2020-07-04 12:40\", \"2020-07-04 12:45\", \"2020-07-04 12:50\", \"2020-07-04 12:55\"]):\n",
        "    ax[i].imshow(dataset[\"data\"].sel(time=d).isel(x=slice(128, 256), y=slice(128, 256)).to_numpy(), cmap='viridis')\n",
        "    ax[i].get_xaxis().set_visible(False)\n",
        "    ax[i].get_yaxis().set_visible(False)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(wspace=0, hspace=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e47848d",
      "metadata": {},
      "source": [
        "From these, your aim is to predict the next 24 images taken five minutes apart for the centre 64x64 region of the 128x128 area. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71a6d2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 12, figsize=(16,3))\n",
        "\n",
        "for i, d in enumerate([\"2020-07-04 13:00\", \"2020-07-04 13:05\", \"2020-07-04 13:10\", \"2020-07-04 13:15\", \"2020-07-04 13:20\", \"2020-07-04 13:25\", \"2020-07-04 13:30\", \"2020-07-04 13:35\", \"2020-07-04 13:40\", \"2020-07-04 13:45\", \"2020-07-04 13:50\", \"2020-07-04 13:55\"]):\n",
        "    ax1[i].imshow(dataset[\"data\"].sel(time=d).isel(x=slice(160, 224), y=slice(160, 224)).to_numpy(), cmap='viridis')\n",
        "    ax1[i].get_xaxis().set_visible(False)\n",
        "    ax1[i].get_yaxis().set_visible(False)\n",
        "\n",
        "for i, d in enumerate([\"2020-07-04 14:00\", \"2020-07-04 14:05\", \"2020-07-04 14:10\", \"2020-07-04 14:15\", \"2020-07-04 14:20\", \"2020-07-04 14:25\", \"2020-07-04 14:30\", \"2020-07-04 14:35\", \"2020-07-04 14:40\", \"2020-07-04 14:45\", \"2020-07-04 14:50\", \"2020-07-04 14:55\"]):\n",
        "    ax2[i].imshow(dataset[\"data\"].sel(time=d).isel(x=slice(160, 224), y=slice(160, 224)).to_numpy(), cmap='viridis')\n",
        "    ax2[i].get_xaxis().set_visible(False)\n",
        "    ax2[i].get_yaxis().set_visible(False)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(wspace=0, hspace=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd438750",
      "metadata": {},
      "source": [
        "## Creating a model\n",
        "\n",
        "### Defining constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37645467",
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 30\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f0d704",
      "metadata": {},
      "source": [
        "\n",
        "### Instantiating the DataLoader\n",
        "\n",
        "Since the dataset is so large, we have provided a `ClimateHackDataset` class in `dataset.py` that selects a sliding window of twelve 128x128 images (taken five minutes apart, corresponding to an hour of data) for randomly sampled areas roughly over the mainland UK to use in training.\n",
        "\n",
        "You do not have to use this at all, if you don't want to. In fact, you are encouraged to customise it as you see fit to better suit your models! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7446ff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Increase crops_per_slice if you want to crop out multiple 128x128 areas from the same sliding window.\n",
        "# To only train on a subset of the dataset, either select what you want and pass it into ClimateHackDataset,\n",
        "# set start_date and end_date to Python datetime objects, or set day_limit > 0 to the number of days from the start to read.\n",
        "\n",
        "ch_dataset = ClimateHackDataset(dataset, crops_per_slice=1, day_limit=7)\n",
        "ch_dataloader = DataLoader(ch_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e017e9",
      "metadata": {},
      "source": [
        "### Instantiating the model\n",
        "\n",
        "Any models you define need to be in the `submission` folder so that they can be evaluated on DOXA. By default, they are defined in `submission/model.py`, which is where this PyTorch model is defined.\n",
        "\n",
        "There are some huge improvements you can make to this basic model, so feel free to use this as a starting point and customise `submission/model.py` to your liking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1759751d",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Model().to(device)\n",
        "optimiser = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = MS_SSIMLoss(channels=24) # produces less blurry images than nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62ee67f",
      "metadata": {},
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b522d66",
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    running_loss = 0\n",
        "    i = 0\n",
        "    count = 0\n",
        "    for batch_coordinates, batch_features, batch_targets in ch_dataloader:\n",
        "        optimiser.zero_grad()\n",
        "        batch_predictions = model(batch_features.to(device))\n",
        "\n",
        "        batch_loss = criterion(batch_predictions.unsqueeze(dim=2), batch_targets.unsqueeze(dim=2).to(device))\n",
        "        batch_loss.backward()\n",
        "\n",
        "        optimiser.step()\n",
        "\n",
        "        running_loss += batch_loss.item() * batch_predictions.shape[0]\n",
        "        count += batch_predictions.shape[0]\n",
        "        i += 1\n",
        "\n",
        "        print(f\"Completed batch {i} of epoch {epoch + 1} with loss {batch_loss.item()} -- processed {count} image sequences ({12 * count} images)\")\n",
        "    \n",
        "    losses.append(running_loss / count)\n",
        "    print(f\"Loss for epoch {epoch + 1}/{EPOCHS}: {losses[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e36642",
      "metadata": {},
      "source": [
        "## Visualising the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73277fa4",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(range(EPOCHS), losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0555a3",
      "metadata": {},
      "source": [
        "## Visualising its predictions\n",
        "\n",
        "Having trained the model, it is often useful to visualise some of its predictions to ensure it is producing a sensible output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ccd168",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = dataset[\"data\"].sel(time=slice(\"2020-07-01 12:00\", \"2020-07-01 12:55\")).isel(x=slice(128, 256), y=slice(128, 256)).to_numpy()\n",
        "y = dataset[\"data\"].sel(time=slice(\"2020-07-01 13:00\", \"2020-07-01 14:55\")).isel(x=slice(160, 224), y=slice(160, 224)).to_numpy()\n",
        "p = model(torch.from_numpy(x.astype(float32)).unsqueeze(dim=0)).detach().numpy()[0]\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 12, figsize=(20,8))\n",
        "\n",
        "# plot the twelve 128x128 input images\n",
        "for i, img in enumerate(x):\n",
        "    ax1[i].imshow(img, cmap='viridis')\n",
        "    ax1[i].get_xaxis().set_visible(False)\n",
        "    ax1[i].get_yaxis().set_visible(False)\n",
        "\n",
        "# plot twelve 64x64 true output images\n",
        "for i, img in enumerate(y[:12]):\n",
        "    ax2[i].imshow(img, cmap='viridis')\n",
        "    ax2[i].get_xaxis().set_visible(False)\n",
        "    ax2[i].get_yaxis().set_visible(False)\n",
        "\n",
        "# plot twelve more 64x64 true output images\n",
        "for i, img in enumerate(y[12:]):\n",
        "    ax3[i].imshow(img, cmap='viridis')\n",
        "    ax3[i].get_xaxis().set_visible(False)\n",
        "    ax3[i].get_yaxis().set_visible(False)\n",
        "\n",
        "# plot the twelve 64x64 predicted output images\n",
        "for i, img in enumerate(p[:12]):\n",
        "    ax4[i].imshow(img, cmap='viridis')\n",
        "    ax4[i].get_xaxis().set_visible(False)\n",
        "    ax4[i].get_yaxis().set_visible(False)\n",
        "\n",
        "# plot twelve more 64x64 output images\n",
        "for i, img in enumerate(p[12:]):\n",
        "    ax5[i].imshow(img, cmap='viridis')\n",
        "    ax5[i].get_xaxis().set_visible(False)\n",
        "    ax5[i].get_yaxis().set_visible(False)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(wspace=0, hspace=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67282866",
      "metadata": {},
      "source": [
        "### Saving the model\n",
        "\n",
        "Having trained the model, it can now be saved in the `submission` folder so that it may be evaluated on [DOXA](https://climatehack.ai/compete)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e223f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'submission/model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1254c4df",
      "metadata": {},
      "source": [
        "## Submitting your model to DOXA\n",
        "\n",
        "Well done -- you made it to the end!\n",
        "\n",
        "You are now ready to submit your model to DOXA!\n",
        "\n",
        "If you are not already logged in, run the following command:\n",
        "\n",
        "```bash\n",
        "$ python doxa_cli.py user login\n",
        "```\n",
        "\n",
        "Finally, to submit your model, upload the entire `submission` folder into which your model was saved using the following command:\n",
        "\n",
        "```bash\n",
        "$ python doxa_cli.py agent upload climatehack ./submission\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Copy of load_and_plot_HRV_UK_Zarr_from_GCS.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6f774df9e68be78be8fedc92c8cad2f0688a777ad163558f0717eecbd1f23d05"
    },
    "kernelspec": {
      "display_name": "zarr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
